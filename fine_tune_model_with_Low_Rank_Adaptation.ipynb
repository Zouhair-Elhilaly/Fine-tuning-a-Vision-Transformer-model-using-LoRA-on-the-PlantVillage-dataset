{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import  torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "98oK0fXLZvJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFYePrhQWCl_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers  torchvision pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, ViTForImageClassification\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "XVmEsL0AAXZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"WinKawaks/vit-tiny-patch16-224\"\n",
        "\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "model_test = ViTForImageClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "IkWEa1xgAhfc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor"
      ],
      "metadata": {
        "id": "Senm7qTXBDiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_test"
      ],
      "metadata": {
        "id": "gQlCv_F-BRuY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "KG-SWyGkJzb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "plt.imshow(image)\n",
        "\n",
        "inputs = processor(images=image, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "42wPsKkSDOQM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j5IMAuYuJ61p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model_test(**inputs)\n",
        "\n",
        "logits = outputs.logits\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "\n",
        "print(model_test.config.id2label[predicted_class_idx])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "l1SNcklpKAZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "# probs\n",
        "topk = torch.topk(probs, k=5)\n",
        "# topk\n",
        "for score, idx in zip(topk.values[0], topk.indices[0]):\n",
        "    print(model_test.config.id2label[idx.item()], score.item())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0LHokTliKNbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_test.config"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NV9-QfkJOevz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JSJRDwITRjpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "FpMUTYmhSoJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/datasets/emmarex/plantdisease\n",
        "!kaggle datasets download -d emmarex/plantdisease"
      ],
      "metadata": {
        "id": "ECxNSJSzS0NQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/plantdisease.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R8FAEcMYS-QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "\n",
        "# 1. Define Transformation (Preprocessing)\n",
        "# We resize to 224x224 and normalize based on ImageNet standards\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 2. Load Dataset from Folders\n",
        "data_dir = '/content/PlantVillage' # Update this path\n",
        "full_dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n"
      ],
      "metadata": {
        "id": "LYGh-_ItPjn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6VpkPJOdsKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset.classes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3oHrW2HxXEXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names from subfolder names\n",
        "class_names = full_dataset.classes\n",
        "print(f\"Detected {len(class_names)} classes: {class_names}\")\n",
        "\n",
        "# 3. Split into Train (80%) and Val (20%)\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_data, val_data = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# 4. Create Data Loaders (The \"Smart\" part for batching)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "JKhxDnTwXDCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "ibiTEUufR18l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(val_data)"
      ],
      "metadata": {
        "id": "v6qAM4PErjxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KPI73BRprpx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  peft datasets accelerate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u77--CRrXpF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = len(class_names)\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    num_labels=NUM_CLASSES\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kzguNhVHX1zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DlU8EgzdYMk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure LoRa"
      ],
      "metadata": {
        "id": "gm9A2Keqj0jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "ukSQ90PVjgXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "collapsed": true,
        "id": "THJzOvoskCHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JissPNofmGu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gxm5W1rPdomF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/DL project/LoRA_ViT-V2\")"
      ],
      "metadata": {
        "id": "YgyDILQgsO9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_loss = float('inf')\n",
        "\n",
        "# for epoch in range(10):\n",
        "#     epoch_loss = 0\n",
        "#     for images, labels in train_loader:\n",
        "#         outputs = model(pixel_values=images, labels=labels)\n",
        "#         loss = outputs.loss\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         epoch_loss += loss.item()\n",
        "\n",
        "#     avg_loss = epoch_loss / len(loader)\n",
        "#     print(f\"Epoch {epoch} Average Loss: {avg_loss}\")\n",
        "\n",
        "#     if avg_loss < best_loss:\n",
        "#         best_loss = avg_loss\n",
        "#         model.save_pretrained(\"best_vit_lora_model\")\n",
        "#         print(\"best model saved\")"
      ],
      "metadata": {
        "id": "V6tl3UYHn8h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "BonRb1FVJTIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FoOrSzbWJ4vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)"
      ],
      "metadata": {
        "id": "0h1ChjvCK5I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize arrays to store losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(10):\n",
        "    # --- TRAINING PHASE ---\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(pixel_values=images, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # --- VALIDATION PHASE ---\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation for efficiency\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(pixel_values=images, labels=labels)\n",
        "            val_loss = outputs.loss\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "    # --- SAVE BEST MODEL BASED ON VALIDATION LOSS ---\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        model.save_pretrained(\"/content/drive/MyDrive/DL project/LoRA_ViT-V2/bestModel\")\n",
        "        print(\" Best model saved (Validation loss improved)\")"
      ],
      "metadata": {
        "id": "bAbs_fZwr7XS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "227432a0"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "# import torch.nn.functional as F\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model = ViTForImageClassification.from_pretrained(model_name ,     num_labels=len(class_names) ,     ignore_mismatched_sizes=True)\n",
        "\n",
        "model_inference = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/DL project/LoRA_ViT-V2/bestModel\")\n",
        "model_inference.to(device)\n",
        "model_inference.eval()\n",
        "\n",
        "def infer_and_display(image_path, model, processor, class_names, device):\n",
        "    # Load the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Preprocess the image\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get logits and probabilities\n",
        "    logits = outputs.logits\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the top predicted class and confidence\n",
        "    top_prob, top_idx = torch.topk(probs, k=1)\n",
        "    predicted_class = class_names[top_idx.item()]\n",
        "    confidence = top_prob.item()\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"Predicted: {predicted_class} (Confidence: {confidence:.2f})\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Predicted Class: {predicted_class}\")\n",
        "    print(f\"Confidence: {confidence:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img = \"/content/PlantVillage/Pepper__bell___healthy/05f73b75-e898-4752-9c9b-ae745994eb01___JR_HL 8801.JPG\"\n",
        "img = \"/content/PlantVillage/Tomato_Bacterial_spot/00a7c269-3476-4d25-b744-44d6353cd921___GCREC_Bact.Sp 5807.JPG\"\n",
        "infer_and_display(img, model_inference, processor, class_names, device)"
      ],
      "metadata": {
        "id": "3lHoC_r_ZDT4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = \"/content/PlantVillage/Pepper__bell___healthy/05f73b75-e898-4752-9c9b-ae745994eb01___JR_HL 8801.JPG\"\n",
        "infer_and_display(img, model_inference, processor, class_names, device)"
      ],
      "metadata": {
        "id": "JpxfSUxBOip7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfm2nkHgO2I7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}